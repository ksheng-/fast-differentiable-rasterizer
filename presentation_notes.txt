dk:
    bezier curves
    linear interpolation
    motivation

ks
    cuda
    one curve does not stress hardware that much, can easily render whole canvas pixel by pixel
    few cuda kernel launches
    certain operations can be slow do to unoptimized memory/cache access patterns

    description:
        naive
        naive w/ cuda - massively increase performance, slightly increased if lerps are done on cpu

        These three basically do minimal cpu processing, sending a giant tensor to cuda, launches a few kernels
        and return

        shrunk is the opposite end of the spectrum, each point gets sent separately and the gradient is calculated in a square around each point. This destroys performance since so much time is spent launching CUDA kernels for relatively cheap operations.

        in order to increase performance, we can simply send less pixels, resulting in fewer FLOPs
        to start, we find the minimum and maximum of the curve to draw a bounding box, only send that to the gpu, and insert it back in.
            this actually decreases performance because, again, the GPU is more than capable of rendering the whole thing, and the additional indexing and processing can be expensive.
        also tried drawing a tight bounding box by rotating the curve so the points align.
            the multiplies and translations required hurt more than help


        HalfTensors (float16) further increases performance, broadcasting replacing expand() also increases performace
            expand tends to have very poor at certain tensor sizes due to cache access patterns
        
        in order to furthur improve resource usage, we introduced the 'tiled' method
            the full area is broken into N*N square tiles
            each of these can be evaluated asynchronously using CUDA streams (the V100 has 80 effective streaming multiprocessors, and we can enqueue our elementwise multiplies on this to execute in order but overlapped
                launch each kernel, execute, wait for sync
            we also do an initial pass to collect the points that (together with its normal dist) cross the boundary of the tile
                this way, we can ignore the tiles that do not contain any points
                and each operation in the tile results in a tile_size * tile_size * points_in_tile operation instead of the original res * res * total_points

FINAL

Slide 1 [title]
Fast differentiable bezier curve rasterizer

[explain curves on board]
What is a bezier curve? Vector curve defined by control points.

What is rasterization? In order to render the curve onto a display with a finite resolution, pixels that fall on the curve must be determined. 
Font glyphs - TrueType fonts are defined as a combination of quadratic bezier curves (with 3 control points each)
Tons of different algorithms are part of every modern graphics pipeline.
Our project - make it differentiable and fast so it can be used as a building block in gradient based methods like optimization / deep learning.
application - reverse rendering, you can use this in a generative model, and find the control points that define a generated glyph.

Slides X [curves]
show curve examples, describe
heres a quadratic bezier - 3 points
heres a cubic bezier - 4 points
describe how to rasterize a bezier curve - either use iterative pixel based methods, or what many modern libraries do (skia etc) simply find points distributed along the curve, and use very hardware optimized straight line drawing algorithms to form an approximate curve. Instead of that, we need gradient information - draw a gaussian at each point and superimpose them to get a smooth, differentiable function with good gradient information.

Benchmark slides
Naive technique as described above is very slow, running on the CPU in vectorized Torch code.
It is also very memory intensive, its doing a lot of unnecessary operations off of the curve.
Solution? only do operations along the curve, on a square around each point. (raster_shrunk), this greatly reduces memory consumption, but is still slow.
Move the tensors onto the GPU, speeds it up a bit.

This is launching so many kernels (proportional to the number of points) and stressing the CPU-GPU connections, that the cost is not really worth it.

GPU is always a tradeoff between embarrasingly parallel operations and the overhead associated with moving data between the CPU and GPU. You want an algorithm that can effectively leverage a few massively parallel GPU operations, and than move it back to the CPU. Also any operations that can be done on the CPU while the GPU is working is essentially free cycles.

test the original naive implementation (one big multiply) in cuda, while finding the points on the CPU
    massive speedup, even with the "wasted" operations

profiled, benchmarked - speed further increased by replacing slow operations (certain calls to expand with certain array sizes perform very poorly do to bad cache/memory access patters, replaced these with much faster array broadcasting ops), and doing all the calculations in half precision. (which also results in lower memory usage by a constant factor)

only need to calculate at the point the region the curve covers-draw a bounding box and calculate only with that
    for many curves, the cost of indexing and finding these bounds is passed by the savings from reducing multiplies
    
slide cubic
but this doesn't really help when the curve covers most of the canvas, and the memory usage still scales horribly.
if you need to render a composition of many bezier curves, you might have to break it into smaller parts

tiled
you want to effectively perform your rasterization at points along the curve, but this incurs a not insignificant cost in finding these regions. How can we do this?
break the canvas in to NxN square tiles
find the points (along with the gradient function) the needs to be rastered in that square, which can be done with a simple bounds check
executed the multiply for each of these in parallel in cuda streams on SM processors
- only need to render in the squares that contain points/gradient information (more squares = tighter bounds but more indexing overhead)
    done asynchrounously so memory usage is much less.
    big performance boost on composite curves / glyphs, still reasonable performance on simple curves
